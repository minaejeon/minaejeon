{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-16T08:05:06.236369Z","iopub.execute_input":"2022-03-16T08:05:06.236679Z","iopub.status.idle":"2022-03-16T08:05:06.265096Z","shell.execute_reply.started":"2022-03-16T08:05:06.236602Z","shell.execute_reply":"2022-03-16T08:05:06.264257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/sf-crime/train.csv.zip\")\ntest=pd.read_csv(\"/kaggle/input/sf-crime/test.csv.zip\")\npd.options.display.max_columns=99\ndisplay(train,test)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T08:05:06.266843Z","iopub.execute_input":"2022-03-16T08:05:06.267114Z","iopub.status.idle":"2022-03-16T08:05:11.004459Z","shell.execute_reply.started":"2022-03-16T08:05:06.26708Z","shell.execute_reply":"2022-03-16T08:05:11.003656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train,test 각 데이터에서 겹치지 않는 값과 정답 값을 drop하여 정제합니다","metadata":{}},{"cell_type":"code","source":"alldata=pd.concat([train,test],axis=0)\nalldata2=alldata.drop(columns=[\"Category\",\"Descript\",\"Resolution\",\"Dates\"])\nalldata2","metadata":{"execution":{"iopub.status.busy":"2022-03-16T08:05:11.005793Z","iopub.execute_input":"2022-03-16T08:05:11.006313Z","iopub.status.idle":"2022-03-16T08:05:11.887589Z","shell.execute_reply.started":"2022-03-16T08:05:11.006272Z","shell.execute_reply":"2022-03-16T08:05:11.886801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"날짜를 숫자로 변경하여 러닝에 용이합니다","metadata":{}},{"cell_type":"code","source":"alldata2[\"DayOfWeek\"].unique()\nalldata2[\"DayOfWeek\"]=alldata2[\"DayOfWeek\"].replace({\"Wednesday\":0, \"Tuesday\":1, \"Monday\":2, \"Sunday\":3, \"Saturday\":4, \"Friday\":5, \"Thursday\":6})\nalldata2[\"DayOfWeek\"]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LabelEncoder를 이용하면 해당 칼럼의 값을 임의의 숫자로 지정해줍니다","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nalldata2[\"PdDistrict\"]=le.fit_transform(alldata2[\"PdDistrict\"])\nalldata2","metadata":{"execution":{"iopub.status.busy":"2022-03-16T08:05:13.270821Z","iopub.execute_input":"2022-03-16T08:05:13.271243Z","iopub.status.idle":"2022-03-16T08:05:14.61631Z","shell.execute_reply.started":"2022-03-16T08:05:13.271206Z","shell.execute_reply":"2022-03-16T08:05:14.615636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PdDistrict의 각 칼럼 고유값에 해당하는 숫자 값을 dictionary로 만들어 줍니다","metadata":{}},{"cell_type":"code","source":"len(le.classes_) #10개\ndict(zip(le.classes_,range(10)))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-16T08:05:14.61768Z","iopub.execute_input":"2022-03-16T08:05:14.618059Z","iopub.status.idle":"2022-03-16T08:05:14.625149Z","shell.execute_reply.started":"2022-03-16T08:05:14.618023Z","shell.execute_reply":"2022-03-16T08:05:14.624359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"칼럼 값이 object인 것이 있는지 추가로 확인합니다.","metadata":{}},{"cell_type":"code","source":"category=alldata2.columns[alldata2.dtypes == object]\ncategory","metadata":{"execution":{"iopub.status.busy":"2022-03-16T08:05:14.626607Z","iopub.execute_input":"2022-03-16T08:05:14.627023Z","iopub.status.idle":"2022-03-16T08:05:14.635761Z","shell.execute_reply.started":"2022-03-16T08:05:14.626987Z","shell.execute_reply":"2022-03-16T08:05:14.635009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Address column에서 레이블 인코딩 처리, 숫자로 변경해 줍니다","metadata":{}},{"cell_type":"code","source":"for col in category:\n    alldata2[col]=le.fit_transform(alldata2[\"Address\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-16T08:05:14.637443Z","iopub.execute_input":"2022-03-16T08:05:14.637771Z","iopub.status.idle":"2022-03-16T08:05:15.350828Z","shell.execute_reply.started":"2022-03-16T08:05:14.637733Z","shell.execute_reply":"2022-03-16T08:05:15.350105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alldata2","metadata":{"execution":{"iopub.status.busy":"2022-03-16T08:05:15.353544Z","iopub.execute_input":"2022-03-16T08:05:15.353827Z","iopub.status.idle":"2022-03-16T08:05:15.370634Z","shell.execute_reply.started":"2022-03-16T08:05:15.35379Z","shell.execute_reply":"2022-03-16T08:05:15.369184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alldata2=alldata2.fillna(-1) # nan값 있을 경우 러닝 과정에서 오류발생\nalldata2","metadata":{"execution":{"iopub.status.busy":"2022-03-16T08:05:15.371526Z","iopub.execute_input":"2022-03-16T08:05:15.372115Z","iopub.status.idle":"2022-03-16T08:05:15.447423Z","shell.execute_reply.started":"2022-03-16T08:05:15.372087Z","shell.execute_reply":"2022-03-16T08:05:15.446634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train2=alldata2[:len(train)]\ntest2=alldata2[len(train):]","metadata":{"execution":{"iopub.status.busy":"2022-03-16T08:05:15.448872Z","iopub.execute_input":"2022-03-16T08:05:15.449157Z","iopub.status.idle":"2022-03-16T08:05:15.453822Z","shell.execute_reply.started":"2022-03-16T08:05:15.449112Z","shell.execute_reply":"2022-03-16T08:05:15.453044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RandomForestClassifier 에서 score 9.53751\nGBDT와 달리 병렬로 트리를 작성. 각 결정 트리의 학습에서 행 데이터나 특징을 샘플링해 전달함으로써 다양한 결정 트리를 작성하고, 앙상블하여 일반화 성능이 높은 예측을 실시. 모델 구축 순서\n1) 학습 데이터에서 행 데이터를 샘플링하여 추출\n2) 1)을 학습하고 결정트리를 작성, 분기를 작성할 때는 특징의 일부만을 샘플링하여 추출하고 특징의 후보로 삼음. 그 후보들로 부터 데이터를 가장 잘 분할하는 특징과 임곗값을 선택해 분기로 삼음\n3) 1)과 2)의 과정을 결정트리의 개수만큼 병렬로 수행\n\n-회귀문제일때는 제곱오차, 분류문제일때는 지니 불순도가 가장 감소하도록 분기를 시행\n-결정트리마다 원래 개수와 같은 수만큼 행 데이터를 복원 추출하는 부트스트랩 샘플링을 실시.\n부트스트랩 샘플링에서는 중복 추출되는 행 데이터가 있는 한편, 평균적으로 행데이터의 1/3 정도는 추출되지 않음\n-분기마다 특징의 일부를 샘플링한 것을 후보로 삼고 그중 분기의 특징을 선택. 회귀 문제에서는 샘플링하지 않고 모든 특징을 후보로 삼음. 분류문제에서는 특징개수의 제곱근 개수만큼 추출하여 후보로 삼음\n\n*결정 트리의 개수와 모델 성능의 관계\n결정 트리를 병렬로 작성하므로, GBDT와는 달리 결정 트리의 개수가 지나치게 증가하여 모델 성능이 낮아지는 일은 없음.\n어느 정도 증가한 후에는 성능이 더 이상 올라가지 않음. 결정 트리의 개수는 계산 시간과 성능의 트레이드 오프로 결정\n\n*예측 확률 타당성\n> 분류 작업시 GBDT에서는 가중치에 기반을 둔 예측 확률의 로그 손실을 최소화하려는 반면, 랜덤 포레스트에서는 지니 불순도를 최소화하려는 각 결정 트리의 예측값의 평균을 구함.\n\n*랜덤 포레스트 방법의 예측 확률 타당성 보증이 되지 않음\n1) 데이터가 충분하지 않을 경우, 0이나 1에 가까운 확률을 예측하기 어려움\n2) 모델이 로그 손실을 최소화 하도록 학습할 경우, 충분한 데이터가 있으면 타당한 확률을 예측하나 그렇지 않으면 왜곡\n> GBDT,신경망, 로지스틱 회귀일때 분류문제에서는 통상적인 설정으로 로그 손실을 목적함수로 삼아 학습하지만 랜덤포레스트에서는 다른 알고리즘으로 분류하므로 확률이 왜곡됨\n","metadata":{}},{"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier \n# rfc=RandomForestClassifier(n_jobs=-1)\n# rfc.fit(train2,train[\"Category\"])\n","metadata":{"execution":{"iopub.status.busy":"2022-03-16T08:05:15.455395Z","iopub.execute_input":"2022-03-16T08:05:15.455674Z","iopub.status.idle":"2022-03-16T08:05:15.462864Z","shell.execute_reply.started":"2022-03-16T08:05:15.45564Z","shell.execute_reply":"2022-03-16T08:05:15.462188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# result=rfc.predict_proba(test2)\n# result","metadata":{"execution":{"iopub.status.busy":"2022-03-16T08:05:15.464176Z","iopub.execute_input":"2022-03-16T08:05:15.465016Z","iopub.status.idle":"2022-03-16T08:05:15.473087Z","shell.execute_reply.started":"2022-03-16T08:05:15.464989Z","shell.execute_reply":"2022-03-16T08:05:15.472344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CatBoostClassifier에서 score 2.41086으로 상승 (score 낮을 수록 순위 상승)\nGBDT의 라이브러리인 CATBOOST는 범주형 변수 지원, 과적합 해소를 위한 정렬된 부스팅, GPU를 활용한 트리모델에서의 부스팅 성능 향상과 같은 기능을 지원\n범주형 변수로서 지정한 특징을 자동으로 타깃 인코딩하여 수치로 변환. 타깃 인코딩을 잘못 사용하면 목적변수의 정보를 부적절하게 사용. 랜덤하게 데이터를 정렬하면서 적용하는 식의 연구 진행. 결정 트리를 작성하는 과정에서 동적으로 범주형 변수의 조합에 대해 타깃 인코딩이 이루어짐. 즉 어떤 분기에서 범주형 변수가 사용되었을 때 그 범주형 변수와 다른 범주형 변수와의 조합에 대해 타깃 인코딩 연산이 이루어지며 급다 깊은 분기에서 그 결과가 사용\n\n데이터 수가 적을 때는 정렬부스팅이라는 알고리즘 사용. 속도는 느리지만 데이터 수가 적을 때 모델 성능이 높음.","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier\ncbc=CatBoostClassifier (task_type=\"GPU\")\ncbc.fit(train2,train[\"Category\"])\n\nresult=cbc.predict_proba(test2)\nresult","metadata":{"execution":{"iopub.status.busy":"2022-03-16T08:05:15.474489Z","iopub.execute_input":"2022-03-16T08:05:15.474796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GBM에 비해 시간은 적게 걸리고, RANDOM FOREST에 비해 성능이 좋다.\n약한 학습기를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 통해 오류를 개선해 나가면서 학습하는 방식이기 때문에**","metadata":{}},{"cell_type":"markdown","source":"GBM(Gradient Boosting Machine)\n부스팅 알고리즘은 여러개의 약한 학습기를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 통해 오류를 개선해 나가면서 학습하는 방식. 랜덤 포레스트보다는 예측성능이 조금 뛰어난 경우가 많음. 그러나 수행시간이 오래 걸리고, 하이퍼 파라미터 튜닝 노력도 더 필요. 약한 학습기의 순차적인 예측 오류 보정을 통해 학습을 수행하므로 멀티 cpu코어 시스템을 사용하더라도 병렬 처리가 되지않아서 대용량 데이터의 경우 학습에 매우 많은 시간이 필요. => 5시간 이상 소요되어 pause","metadata":{}},{"cell_type":"code","source":"# from sklearn.ensemble import GradientBoostingClassifier\n# gbc=GradientBoostingClassifier(random_state=0)\n# gbc.fit(train2,train[\"Category\"])\n# result=gbc.predict_proba(test2)\n# result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub=pd.read_csv(\"/kaggle/input/sf-crime/sampleSubmission.csv.zip\")\nsub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.iloc[:,1:]=result\nsub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RandomForestClassifier를 사용하면 score가 9.53751 이었으나,CatBoostClassifier를 사용하면 score 2.41086으로 개선되었습니다. (score 낮을 수록 순위가 상승합니다)\nRandomForestClassifier는 로그 손실을 최소화 하는 방향이나 과정에서 예측 오류에 대한 가중치를 적용하지 않습니다.\nCatBoostClassifier는 예측 과정에서 가중치를 적용하여 오류를 보정하기 때문에 시간은 많이 들지만 더 정확한 예측이 가능합니다.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(\"cbc.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}